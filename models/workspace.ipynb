{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b11b35",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "- [Load Dataset](#load-dataset)\n",
    "- [Preprocess](#preprocess)\n",
    "- [Prepare Data](#prepare-data)\n",
    "- [Build Model](#build-model)\n",
    "- [Train Model](#train-model)\n",
    "- [Generate Jokes](#generate-jokes)\n",
    "- [Evaluate & Tune](#evaluate--tune)\n",
    "- [Save & Deploy](#save--deploy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c9a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saher/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = load_dataset(\"diwank/good_joke-dataset\")\n",
    "df = df[\"train\"].to_pandas() # since we are only interested in the training set, then convert it to pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a3949",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Use HuggingFace `datasets` library to load `diwank/good_joke-dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1dd67a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (20045, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20045 entries, 0 to 20044\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   source    20045 non-null  object \n",
      " 1   body      20045 non-null  object \n",
      " 2   title     20045 non-null  object \n",
      " 3   category  20045 non-null  object \n",
      " 4   rating    20045 non-null  float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 783.1+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape:\", df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f24897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wocka</td>\n",
       "      <td>What do you call a cow with no legs?\\nGround B...</td>\n",
       "      <td>Cow With No Legs</td>\n",
       "      <td>Animal</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wocka</td>\n",
       "      <td>What's black and white and red all over?\\nA ne...</td>\n",
       "      <td>Black, White and Red</td>\n",
       "      <td>Other / Misc</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wocka</td>\n",
       "      <td>There's this dyslexic guy... he walked into a ...</td>\n",
       "      <td>Into the Bar</td>\n",
       "      <td>Bar</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wocka</td>\n",
       "      <td>There was a dyslexic insomniac agnostic.\\nHe l...</td>\n",
       "      <td>Pondering the afterlife</td>\n",
       "      <td>One Liners</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wocka</td>\n",
       "      <td>What do you call 500 lawyers at the bottom of ...</td>\n",
       "      <td>500 Lawyers</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                               body  \\\n",
       "0  wocka  What do you call a cow with no legs?\\nGround B...   \n",
       "1  wocka  What's black and white and red all over?\\nA ne...   \n",
       "2  wocka  There's this dyslexic guy... he walked into a ...   \n",
       "3  wocka  There was a dyslexic insomniac agnostic.\\nHe l...   \n",
       "4  wocka  What do you call 500 lawyers at the bottom of ...   \n",
       "\n",
       "                     title      category  rating  \n",
       "0         Cow With No Legs        Animal     4.0  \n",
       "1     Black, White and Red  Other / Misc     4.0  \n",
       "2             Into the Bar           Bar     4.0  \n",
       "3  Pondering the afterlife    One Liners     4.0  \n",
       "4              500 Lawyers        Lawyer     4.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fef52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "                   19451\n",
       "Insults              109\n",
       "One Liners            96\n",
       "Miscellaneous         66\n",
       "Yo Mama               43\n",
       "Other / Misc          38\n",
       "Yo Momma              33\n",
       "Light Bulbs           27\n",
       "Puns                  26\n",
       "Animal                18\n",
       "Lightbulb             11\n",
       "Medical               11\n",
       "Knock-Knock           10\n",
       "Redneck                9\n",
       "Bar                    9\n",
       "Religious              8\n",
       "Children               7\n",
       "Sex                    6\n",
       "Political              6\n",
       "Lawyer                 5\n",
       "Sports                 5\n",
       "Men / Women            5\n",
       "Computers              4\n",
       "Tech                   3\n",
       "Women                  3\n",
       "Blond                  3\n",
       "At Work                3\n",
       "Blonde Jokes           3\n",
       "Deep Thoughts          3\n",
       "Heaven and Hell        3\n",
       "Family, Parents        3\n",
       "Animals                2\n",
       "Men                    2\n",
       "College                2\n",
       "Gross                  2\n",
       "Military               2\n",
       "Police Jokes           2\n",
       "News / Politics        1\n",
       "Idiots                 1\n",
       "Business               1\n",
       "Music                  1\n",
       "Crazy Jokes            1\n",
       "Science                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fdd0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top categories: ['']\n"
     ]
    }
   ],
   "source": [
    "# get the top 1 categories\n",
    "top_categories = df['category'].value_counts().head(1).index.tolist()\n",
    "print(\"Top categories:\", top_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559a0316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "                                                           286\n",
       "Why did the chicken cross the road?                         15\n",
       "Knock knock                                                 13\n",
       "How do you make holy water?                                 12\n",
       "What did the leper say to the prostitute?                   11\n",
       "What do you call a cow with no legs?                        11\n",
       "How many feminists does it take to change a light bulb?     10\n",
       "How many tickles does it take to make an octopus laugh?      9\n",
       "Why was six afraid of seven?                                 9\n",
       "Knock Knock                                                  9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38fa41",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "- Clean text (lowercase, strip extra spaces).\n",
    "- Tokenize jokes (split into sequences).\n",
    "- Build vocabulary & encode tokens to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89e20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class JokePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\n', '', text)  # Remove newlines\n",
    "        text = re.sub(r'\\t+', ' ', text)  # Remove tabs\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+$', '', text)  # Remove trailing spaces\n",
    "        text = re.sub(r'^\\s+', '', text)  # Remove leading spaces\n",
    "        return text.strip()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Simple whitespace tokenizer\n",
    "        return text.split()\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # Build vocabulary from a list of texts\n",
    "        tokens = set()\n",
    "        for text in texts:\n",
    "            tokens.update(self.tokenize(self.clean_text(text)))\n",
    "        self.vocab = sorted(tokens)\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.idx2token = {idx: token for token, idx in self.token2idx.items()} \n",
    "\n",
    "    def encode(self, text):\n",
    "        # Encode text to list of token indices\n",
    "        tokens = self.tokenize(self.clean_text(text))\n",
    "        return [self.token2idx[token] for token in tokens if token in self.token2idx]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        # Decode list of indices to text\n",
    "        return ' '.join([self.idx2token[idx] for idx in indices if idx in self.idx2token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da340083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        What do you call a cow with no legs?\\nGround Beef!\n",
       "1                                                    What's black and white and red all over?\\nA newspaper.\n",
       "2                                                      There's this dyslexic guy... he walked into a bra...\n",
       "3    There was a dyslexic insomniac agnostic.\\nHe laid awake all night wondering if there really was a Dog.\n",
       "4                                          What do you call 500 lawyers at the bottom of the sea?\\nA start.\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show full body of jokes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df['body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dadfc201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do you call a cow with no legs?\\nGround Beef!</td>\n",
       "      <td>what do you call a cow with no legs? ground beef!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's black and white and red all over?\\nA newspaper.</td>\n",
       "      <td>what's black and white and red all over? a newspaper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There's this dyslexic guy... he walked into a bra...</td>\n",
       "      <td>there's this dyslexic guy... he walked into a bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was a dyslexic insomniac agnostic.\\nHe laid awake all night wondering if there really was a Dog.</td>\n",
       "      <td>there was a dyslexic insomniac agnostic. he laid awake all night wondering if there really was a dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you call 500 lawyers at the bottom of the sea?\\nA start.</td>\n",
       "      <td>what do you call 500 lawyers at the bottom of the sea? a start.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     body  \\\n",
       "0                                                      What do you call a cow with no legs?\\nGround Beef!   \n",
       "1                                                  What's black and white and red all over?\\nA newspaper.   \n",
       "2                                                    There's this dyslexic guy... he walked into a bra...   \n",
       "3  There was a dyslexic insomniac agnostic.\\nHe laid awake all night wondering if there really was a Dog.   \n",
       "4                                        What do you call 500 lawyers at the bottom of the sea?\\nA start.   \n",
       "\n",
       "                                                                                              body_clean  \n",
       "0                                                      what do you call a cow with no legs? ground beef!  \n",
       "1                                                  what's black and white and red all over? a newspaper.  \n",
       "2                                                   there's this dyslexic guy... he walked into a bra...  \n",
       "3  there was a dyslexic insomniac agnostic. he laid awake all night wondering if there really was a dog.  \n",
       "4                                        what do you call 500 lawyers at the bottom of the sea? a start.  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate preprocessor and clean the 'body' column\n",
    "joke_preprocessor = JokePreprocessor()\n",
    "df['body_clean'] = df['body'].apply(joke_preprocessor.clean_text)\n",
    "\n",
    "# Build vocabulary on cleaned jokes\n",
    "joke_preprocessor.build_vocab(df['body_clean'])\n",
    "\n",
    "# Show a sample of cleaned data\n",
    "df[['body', 'body_clean']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4e6e6",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "- Convert cleaned jokes to sequences of token indices.\n",
    "- Prepare input and target sequences for training (next-word prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07654676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 21:23:40.779420: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-28 21:23:40.912534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751135020.948607   15040 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751135020.959498   15040 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751135021.040430   15040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751135021.040469   15040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751135021.040473   15040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751135021.040476   15040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-28 21:23:41.052726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25522\n",
      "Max sequence length: 34\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert cleaned jokes to sequences of token indices\n",
    "sequences = [joke_preprocessor.encode(joke) for joke in df['body_clean'] if len(joke_preprocessor.encode(joke)) > 1] # Filter out empty jokes\n",
    "\n",
    "# Prepare input and target sequences\n",
    "input_seqs = []\n",
    "target_seqs = []\n",
    "for seq in sequences: \n",
    "    for i in range(1, len(seq)):\n",
    "        input_seqs.append(seq[:i])\n",
    "        target_seqs.append(seq[i])\n",
    "\n",
    "# Pad input sequences\n",
    "max_seq_len = max([len(seq) for seq in input_seqs])\n",
    "input_seqs_padded = pad_sequences(input_seqs, maxlen=max_seq_len, padding='pre') #make pre-padding for the input sequences that has a less length than the max sequence length\n",
    "target_seqs = np.array(target_seqs)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_seqs_padded, target_seqs, test_size=0.1, random_state=42)\n",
    "vocab_size = len(joke_preprocessor.vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Max sequence length:\", max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f1b31",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "- Use a simple RNN (LSTM) for next-word prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16647508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saher/miniconda3/envs/ml/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_len),\n",
    "    LSTM(hidden_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92abf32",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6131d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751135225.387242   15568 cuda_dnn.cc:529] Loaded cuDNN version 90701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.0357 - loss: 8.0158 - val_accuracy: 0.0436 - val_loss: 7.5466\n",
      "Epoch 2/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.0538 - loss: 7.1213 - val_accuracy: 0.0764 - val_loss: 7.3168\n",
      "Epoch 3/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.0851 - loss: 6.6236 - val_accuracy: 0.1013 - val_loss: 7.1577\n",
      "Epoch 4/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.1154 - loss: 6.1628 - val_accuracy: 0.1144 - val_loss: 7.1094\n",
      "Epoch 5/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.1340 - loss: 5.7707 - val_accuracy: 0.1221 - val_loss: 7.1411\n",
      "Epoch 6/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1513 - loss: 5.4200 - val_accuracy: 0.1301 - val_loss: 7.1833\n",
      "Epoch 7/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1680 - loss: 5.1050 - val_accuracy: 0.1354 - val_loss: 7.2417\n",
      "Epoch 8/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.1824 - loss: 4.8225 - val_accuracy: 0.1427 - val_loss: 7.2997\n",
      "Epoch 9/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.2069 - loss: 4.5393 - val_accuracy: 0.1452 - val_loss: 7.3772\n",
      "Epoch 10/10\n",
      "\u001b[1m1025/1025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.2330 - loss: 4.2770 - val_accuracy: 0.1479 - val_loss: 7.4840\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e178722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c92bf3",
   "metadata": {},
   "source": [
    "### Generate Jokes\n",
    "- Function to generate joke completion given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(prompt, max_gen_len=30):\n",
    "    # Clean and encode the prompt\n",
    "    cleaned = joke_preprocessor.clean_text(prompt)\n",
    "    encoded = joke_preprocessor.encode(cleaned)\n",
    "    for _ in range(max_gen_len):\n",
    "        padded = pad_sequences([encoded], maxlen=max_seq_len, padding='pre')\n",
    "        pred_probs = model.predict(padded, verbose=0)[0]\n",
    "        next_idx = np.argmax(pred_probs)\n",
    "        encoded.append(next_idx)\n",
    "        # Stop if end of joke (optional: if next_idx is for a special token)\n",
    "    return joke_preprocessor.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217e608",
   "metadata": {},
   "source": [
    "### Try Joke Completion!\n",
    "- Enter a joke prompt and let the model complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e11cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Knock knock\"\n",
    "completed_joke = generate_joke(prompt)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated joke:\", completed_joke)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
